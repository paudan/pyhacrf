{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long input strings\n",
    "\n",
    "For certain tasks it might make more sense to tokenize input strings first and then extract features on these string lists rather than on the original character lists.\n",
    "\n",
    "To demonstrate this I'll take some example strings from [highered](https://github.com/datamade/highered/) and learn models using these two feature extraction techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [(u'caring hands a step ahead', u'el valor little tykes ii'),\n",
    "  (u'dulles', u\"chicago public schools o'keeffe, isabell c.\"),\n",
    "  (u'erie neighborhood house fcch-carmen l. vega site',\n",
    "   u'erie neighborhood house fcch-servia galva site'),\n",
    "  (u'chicago public schools dvorak math & science tech academy, anton',\n",
    "   u'chicago public schools perez, manuel'),\n",
    "  (u'v & j day care center', u\"henry booth house granny's day care center\"),\n",
    "  (u'home of life community dev. corp. - home of life just for you',\n",
    "   u'urban family and community centers'),\n",
    "  (u'carole robertson center for learning fcch-ileana gonzalez',\n",
    "   u'carole robertson center for learning fcch-rhonda culverson'),\n",
    "  (u'bethel new life bethel child development',\n",
    "   u'mary crane league mary crane center (lake & pulaski)'),\n",
    "  (u'easter seals society of metropolitan chicago - stepping stones early/childhood lear',\n",
    "   u\"marcy newberry association kenyatta's day care\"),\n",
    "  (u'westside holistic family services westside holistic family services',\n",
    "   u'childserv lawndale'),\n",
    "  \n",
    "  (u'higgins', u'higgins'),\n",
    "  (u'ymca south side', u'ymca of metropolitan chicago - south side ymca'),\n",
    "  (u'chicago commons association paulo freire',\n",
    "   u'chicago commons association paulo freire'),\n",
    "  (u'fresh start daycare, inc.',\n",
    "   u'easter seals society of metropolitan chicago fresh start day care center'),\n",
    "  (u'el valor teddy bear 3', u'teddy bear 3'),\n",
    "  (u'chicago child care society chicago child care society',\n",
    "   u'chicago child care society-child and family dev center'),\n",
    "  (u'hull house - uptown', u'uptown family care center')]\n",
    "Y = [u'distinct',\n",
    "  u'distinct',\n",
    "  u'distinct',\n",
    "  u'distinct',\n",
    "  u'distinct',\n",
    "  u'distinct',\n",
    "  u'distinct',\n",
    "  u'distinct',\n",
    "  u'distinct',\n",
    "  u'distinct',\n",
    "  u'match',\n",
    "  u'match',\n",
    "  u'match',\n",
    "  u'match',\n",
    "  u'match',\n",
    "  u'match',\n",
    "  u'match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhacrf import StringPairFeatureExtractor, Hacrf\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "feature_extractor = StringPairFeatureExtractor(match=True, numeric=True)\n",
    "X_extracted = feature_extractor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  Log-likelihood |gradient|\n",
      "         0     -11.78      652.6\n",
      "         1     -609.0  1.575e+03\n",
      "         2     -54.72  1.571e+03\n",
      "         3     -11.31      563.1\n",
      "         4     -10.83      144.8\n",
      "         5     -10.78      120.7\n",
      "         6      -10.7      146.0\n",
      "         7     -10.43      252.0\n",
      "         8     -10.13      331.2\n",
      "         9     -9.795      253.3\n",
      "        10      -9.57      104.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyhacrf.pyhacrf.Hacrf at 0x7fe671b7af28>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%timeit -n1 -r1\n",
    "# Train model\n",
    "model = Hacrf(l2_regularization=1.0, optimizer=fmin_l_bfgs_b, optimizer_kwargs={'maxfun': 10})\n",
    "model.fit(X_extracted, Y, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 8]\n",
      " [3 4]]\n",
      "[[0.6172364  0.3827636 ]\n",
      " [0.31317953 0.68682047]\n",
      " [0.62107204 0.37892796]\n",
      " [0.85327835 0.14672165]\n",
      " [0.43888254 0.56111746]\n",
      " [0.85518886 0.14481114]\n",
      " [0.66413495 0.33586505]\n",
      " [0.59691953 0.40308047]\n",
      " [0.9149792  0.0850208 ]\n",
      " [0.91642262 0.08357738]\n",
      " [0.48684886 0.51315114]\n",
      " [0.3672367  0.6327633 ]\n",
      " [0.56645306 0.43354694]\n",
      " [0.32613658 0.67386342]\n",
      " [0.62561551 0.37438449]\n",
      " [0.65620343 0.34379657]\n",
      " [0.55173875 0.44826125]]\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -n1 -r1\n",
    "# Evaluate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "predictions = model.predict(X_extracted)\n",
    "print(confusion_matrix(Y, predictions))\n",
    "print(model.predict_proba(X_extracted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhacrf import PairFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokX = [[sentence.split(' ') for sentence in pair] for pair in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<lambda>() missing 2 required positional arguments: 's1' and 's2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5250ef89babd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#  standard edit distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mfeature_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPairFeatureExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mX_extracted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyhacrf/feature_extraction.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_X, y)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mFeature\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muse\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mestimators\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfurther\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyhacrf/feature_extraction.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_X, y)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mFeature\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muse\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mestimators\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfurther\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \"\"\"\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msequence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_X\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyhacrf/feature_extraction.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mFeature\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muse\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mestimators\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfurther\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \"\"\"\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msequence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_X\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyhacrf/feature_extraction.py\u001b[0m in \u001b[0;36m_extract_features\u001b[0;34m(self, sequence1, sequence2)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_function\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_binary_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mfeature_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: <lambda>() missing 2 required positional arguments: 's1' and 's2'"
     ]
    }
   ],
   "source": [
    "real = [\n",
    "    lambda i, j, s1, s2: 1.0,\n",
    "    lambda i, j, s1, s2: 1.0 if s1[i] == s2[j] else 0.0,\n",
    "    lambda i, j, s1, s2: 1.0 if s1[i] == s2[j] and len(s1[i]) >= 6 else 0.0,\n",
    "    lambda i, j, s1, s2: 1.0 if s1[i].isdigit() and s2[j].isdigit() and s1[i] == s2[j] else 0.0,\n",
    "    lambda i, j, s1, s2: 1.0 if s1[i].isalpha() and s2[j].isalpha() and s1[i] == s2[j] else 0.0,\n",
    "    lambda i, j, s1, s2: 1.0 if not s1[i].isalpha() and not s2[j].isalpha() else 0.0\n",
    "]\n",
    "# Other ideas are:\n",
    "#  to look up whether words are dictionary words,\n",
    "#  longest common subsequence,\n",
    "#  standard edit distance\n",
    "feature_extractor = PairFeatureExtractor(real=real)\n",
    "X_extracted = feature_extractor.fit_transform(tokX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit -n1 -r1\n",
    "# Train model\n",
    "model = Hacrf(l2_regularization=1.0, optimizer=fmin_l_bfgs_b, optimizer_kwargs={'maxfun': 400})\n",
    "model.fit(X_extracted, Y, verbosity=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "# Evaluate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "predictions = model.predict(X_extracted)\n",
    "print(confusion_matrix(Y, predictions))\n",
    "print(model.predict_proba(X_extracted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit distance and word frequency features\n",
    "\n",
    "Let's also add the the Levenschtein distance as a features. \n",
    "\n",
    "When we peek at the training examples, it looks as if less common words should be more informative of a match - let's add a feature for the word frequency as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "editdistance.eval('cheese', 'kaas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokX = [[sentence.split(' ') for sentence in pair] for pair in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = [\n",
    "    lambda i, j, s1, s2: 1.0,\n",
    "    lambda i, j, s1, s2: 1.0 if s1[i] == s2[j] else 0.0,\n",
    "    lambda i, j, s1, s2: 1.0 if s1[i].isdigit() and s2[j].isdigit() and s1[i] == s2[j] else 0.0,\n",
    "    lambda i, j, s1, s2: 1.0 if not s1[i].isalpha() and not s2[j].isalpha() else 0.0,\n",
    "    lambda i, j, s1, s2: editdistance.eval(s1[i], s2[j]),\n",
    "    lambda i, j, s1, s2: np.log(editdistance.eval(s1[i], s2[j]) + 1),\n",
    "    lambda i, j, s1, s2: (editdistance.eval(s1[i], s2[j])) / max(len(s1[i]), len(s2[j])),\n",
    "    lambda i, j, s1, s2: 1.0 - (editdistance.eval(s1[i], s2[j])) / max(len(s1[i]), len(s2[j]))\n",
    "]\n",
    "# Other ideas are:\n",
    "#  to look up whether words are dictionary words,\n",
    "#  longest common subsequence,\n",
    "#  standard edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "errors_val = []\n",
    "errors_train = []\n",
    "for i, featureset in enumerate([[0, 1],\n",
    "                                [0, 1, 2],\n",
    "                                [0, 1, 2, 3],\n",
    "                                [0, 4], \n",
    "                                [0, 1, 4], \n",
    "                                [0, 1, 2, 3, 4],\n",
    "                                [0, 5],\n",
    "                                [0, 1, 5],\n",
    "                                [0, 1, 2, 3, 5],\n",
    "                                [0, 6],\n",
    "                                [0, 1, 6],\n",
    "                                [0, 1, 2, 3, 6],\n",
    "                                [0, 7],\n",
    "                                [0, 1, 7],\n",
    "                                [0, 1, 2, 3, 7]]):\n",
    "    print '{:4}{:18}'.format(i, featureset),\n",
    "    errs_val = []\n",
    "    errs_train = []\n",
    "    for repeat in xrange(15):\n",
    "        x_train, x_val, y_train, y_val = train_test_split(tokX, Y, test_size=0.2)\n",
    "        feature_extractor = PairFeatureExtractor(real=[real[f] for f in featureset])\n",
    "        X_extracted = feature_extractor.fit_transform(x_train)\n",
    "\n",
    "        model = Hacrf(l2_regularization=1.0, optimizer=fmin_l_bfgs_b, optimizer_kwargs={'maxfun': 400})\n",
    "        model.fit(X_extracted, y_train)\n",
    "        \n",
    "        predictions = model.predict(X_extracted)\n",
    "        err_train = 1.0 - accuracy_score(y_train, predictions)\n",
    "        \n",
    "        X_extracted = feature_extractor.transform(x_val)\n",
    "        predictions = model.predict(X_extracted)\n",
    "        err_val = 1.0 - accuracy_score(y_val, predictions)\n",
    "        if repeat % 10 == 0:\n",
    "            print '{:.2f}'.format(err_train),\n",
    "            print '{:.2f}'.format(err_val),\n",
    "        errs_val.append(err_val)\n",
    "        errs_train.append(err_train)\n",
    "    print '  => {:.2f} +- {:.2f} | {:.2f} +- {:.2f}'.format(np.average(errs_train), \n",
    "                                                            np.std(errs_train),\n",
    "                                                            np.average(errs_val), \n",
    "                                                            np.std(errs_val))\n",
    "    errors_train.append(errs_train)\n",
    "    errors_val.append(errs_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "It seems that tokenising the text not only speeds up training and scoring by 40x, it also improves the predictions. We definitely need more data to do this properly though."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
